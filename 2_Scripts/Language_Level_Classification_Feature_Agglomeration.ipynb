{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Level Classification with Feature Agglomeration\n",
    "\n",
    "----\n",
    "\n",
    "Project: Language Level Analysis and Classification <br>\n",
    "Seminar *Educational Assessment for Language Technology* <br>\n",
    "WS 2015/16, Magdalena Wolska\n",
    "\n",
    "\n",
    "Julia Suter, January 2018\n",
    "\n",
    "\n",
    "----\n",
    "Language_Level_Classification_Feature_Agglomeration.ipynb\n",
    "\n",
    "- train, test and evaluate language level classifier with feature agglomeration on different settings\n",
    "- identify sparse / less relevant features\n",
    "- feature clustering\n",
    "- parameter screening (including cluster size)\n",
    "- visualize feature clusters, feature relevance, and performance on differnet cluster sizes (if enabled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster\n",
    "import sklearn\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter('ignore', FutureWarning)\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature Cluster Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot for showing feature clusters\n",
    "\n",
    "def line_plot(clusters, names, colors):\n",
    "    \"\"\"Plot feature clusters.\n",
    "    \n",
    "    Args:       clusters (np.array)\n",
    "                names (np.array)\n",
    "                colors (list)\n",
    "    \"\"\"\n",
    "\n",
    "    # Init\n",
    "    fig = plt.figure(figsize=(18, 0.5))\n",
    "\n",
    "    # Plot lines of appropriate length\n",
    "    # Note 1: The \"overhangs\" may need manual adjustment to look nice.\n",
    "    cluster_sizes = np.bincount(clusters)\n",
    "    names_rdy = names.tolist()\n",
    "        \n",
    "    pos = 0.0\n",
    "    for clust_size, color in zip(cluster_sizes, colors):\n",
    "        plt.plot([pos-0.9, pos+clust_size-1+0.1], [0, 0], lw=7, c=color)\n",
    "        pos += clust_size+1\n",
    "        names_rdy.insert(int(pos-1),\"\")\n",
    "\n",
    "            \n",
    "    # Get axis\n",
    "    ax = fig.gca()\n",
    "\n",
    "    # Remove all the frame stuff\n",
    "    ax.set_frame_on(False)\n",
    "    ax.xaxis.set_ticks_position('none') \n",
    "    ax.yaxis.set_visible(False)\n",
    "    \n",
    "    # Set the labels\n",
    "    ax.set_xticks(range(len(names_rdy)))\n",
    "    ax.xaxis.set_ticklabels(names_rdy, rotation=55, ha=\"right\", fontsize=11)\n",
    "    \n",
    "    ax.set_xlim((-0.9, pos+clust_size-1+0.1))\n",
    "\n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings\n",
      "-----------\n",
      "Version: default\n",
      "Baseline: False\n",
      "Data from: /Literature_Features_short/\n"
     ]
    }
   ],
   "source": [
    "version = 'default'     # or: default_nopara, non_linear_rbf\n",
    "baseline = False\n",
    "version = 'baseline' if baseline else version\n",
    "\n",
    "# use literary texts from Gutenberg instead of Language Levels\n",
    "literature_version = False\n",
    "\n",
    "# get data dir\n",
    "data_dir = '../3_Text_features/Features/'\n",
    "#data_dir = '../3_Text_features/Features_truncated_beginning/'\n",
    "#data_dir = '../3_Text_features/Features_truncated_middle/'\n",
    "# = '../3_Text_features/Features_5sents_chunks/'\n",
    "\n",
    "if literature_version:\n",
    "    data_dir = '../3_Text_features/Literature_Features/'\n",
    "    #data_dir = '../3_Text_features/Literature_Features_short/'\n",
    "\n",
    "find_cluster_n = False\n",
    "para_screening = False\n",
    "\n",
    "# constants\n",
    "RANDOM_STATE = 41\n",
    "TRAIN_SIZE = 0.9\n",
    "NUMBER_OF_FEATURES = None\n",
    "MIN_N_WORDS = 0\n",
    "MIN_N_SENTS = 3\n",
    "N_REL_FEATURES = 15\n",
    "\n",
    "# use different training size for literature set\n",
    "if literature_version:\n",
    "    TRAIN_SIZE = 0.8\n",
    "    \n",
    "sparsity_selected = False  # not used\n",
    "sparsity = False\n",
    "rel_features = True\n",
    "\n",
    "feat_agglo = True\n",
    "n_clusters = 5\n",
    "\n",
    "only_non_agglo_f = False\n",
    "only_agglo_f = False\n",
    "\n",
    "print('Settings\\n-----------')\n",
    "print('Version: {}'.format(version))\n",
    "print('Baseline: {}'.format(baseline))\n",
    "print('Data from: {}'.format(data_dir[18:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for assigning labels\n",
    "label_dict = {0:'A1',1:'B1',2:'B2',3:'A2'}\n",
    "\n",
    "# collect data\n",
    "all_feature_arrays = []\n",
    "solutions = []\n",
    "all_sents_n = []\n",
    "\n",
    "total_files_original = 0\n",
    "\n",
    "# for each dir in [A1,A2,B1,B2]\n",
    "for i,directory in enumerate(os.listdir(data_dir)):\n",
    "\n",
    "    # get number of files\n",
    "    all_files = os.listdir(data_dir+directory) \n",
    "    total_files_original += len(all_files)\n",
    "\n",
    "    # get number of features for feature array\n",
    "    if baseline:\n",
    "        NUMBER_OF_FEATURES = 2\n",
    "    else:\n",
    "        sample_file = all_files[0]\n",
    "        df = pd.read_csv(data_dir+directory+'/'+sample_file)\n",
    "        array = np.array(df)        \n",
    "        NUMBER_OF_FEATURES = array.shape[0]-4\n",
    "        \n",
    "    # set feature array\n",
    "    feature_array = np.zeros((len(all_files), NUMBER_OF_FEATURES))\n",
    "\n",
    "    # for each file in subfolder\n",
    "    for j, file in enumerate(all_files):\n",
    "\n",
    "        # read data in pandas df (just for fun)\n",
    "        df = pd.read_csv(data_dir+directory+'/'+file)\n",
    "\n",
    "        # transform into array\n",
    "        array = np.array(df)\n",
    "\n",
    "        # get values and names\n",
    "        features = array[:,1]         \n",
    "        feature_names = array[:,0]\n",
    "\n",
    "        # get number of words and sents\n",
    "        n_words = features[2]\n",
    "        n_sents = features[3]\n",
    "        \n",
    "        # FILTERING if needed\n",
    "        if n_words < MIN_N_WORDS:\n",
    "            continue\n",
    "        if n_sents < MIN_N_SENTS:\n",
    "            continue\n",
    "\n",
    "        # get all features (except words per sent, LIX, opposite if baseline)\n",
    "        # n_sents, n_words is never used\n",
    "        features = features[:2] if baseline else features[4:]\n",
    "        feature_names = features_names[:2] if baseline else feature_names[4:]\n",
    "\n",
    "        # IF ONLY RELEVANT FEATURES\n",
    "        #features = features[:2] if baseline else features[4+27:]\n",
    "        #features = np.delete(features,-4)\n",
    "        #feature_names = np.delete(feature_names,-4)\n",
    "\n",
    "        # save features in array\n",
    "        feature_array[j] = features\n",
    "        \n",
    "        \n",
    "    # if FILTERING remove empty rows\n",
    "    feature_array = feature_array[~(feature_array==0).all(1)]\n",
    "\n",
    "    # add correct label to solution dict (x times; x = number of samples)\n",
    "    solutions.extend([directory]*feature_array.shape[0]) \n",
    "    \n",
    "    # append feature array \n",
    "    all_feature_arrays.append(feature_array)\n",
    "\n",
    "    \n",
    "# concatenate feature arrays\n",
    "feature_array = np.concatenate(all_feature_arrays)\n",
    "# transform solution array\n",
    "solution_array = np.array(solutions)            \n",
    "\n",
    "# print for fun\n",
    "print('# samples: {}'.format(feature_array.shape[0]))\n",
    "print('# features: {}'.format(feature_array.shape[1]))\n",
    "\n",
    "# how many were filtered out\n",
    "print('# filtered out docs: {}'.format(total_files_original-feature_array.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Agglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_agglomeration(n_clusters, feature_array_s, feature_array_n, s_names, n_names, \n",
    "                          only_agglo_f=False, only_non_agglo_f=False):\n",
    "    \"\"\"Feature agglomeration: reduce feature space (feature_array_s) to a specific number of features\n",
    "    \n",
    "    Args:       n_clusters (int)\n",
    "                feature_array_s (np.array)   # sparse/non-relevant\n",
    "                feature_array_n (np.array)   # non-sparse/relevant\n",
    "                s_names (np.array)\n",
    "                n_names (np.array)\n",
    "                only_agglo_f (Boolean)\n",
    "                only_non_agglo_f (Boolean)\n",
    "                 \n",
    "    Returns:    feature_array_final (np.array)\n",
    "                f_names (np.array)\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    # Scale features\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    feature_array_s = scaler.fit_transform(feature_array_s)\n",
    "    \n",
    "    # if only relevant (or non-sparse) features are used\n",
    "    if only_non_agglo_f:\n",
    "        feature_array_n = scaler.fit_transform(feature_array_n)\n",
    "\n",
    "    # define cluster\n",
    "    cluster = sklearn.cluster.FeatureAgglomeration(n_clusters=n_clusters, affinity='l2',\n",
    "                                               connectivity=None, compute_full_tree='auto', \n",
    "                                               linkage='average',\n",
    "                                               pooling_func=np.sum)\n",
    "    \n",
    "    # if only relevant (or non-sparse) features are feature agglomerated\n",
    "    if only_non_agglo_f:\n",
    "        \n",
    "        \n",
    "        # fit data\n",
    "        cluster.fit(feature_array_n)\n",
    "        X_reduced = cluster.transform(feature_array_n)\n",
    "\n",
    "        # examine labels\n",
    "        labels = cluster.labels_\n",
    "        argsort_labels = np.argsort(labels)\n",
    "        \n",
    "        # assign agglomerated features to feature array\n",
    "        feature_array_n = X_reduced\n",
    "       \n",
    "    # if sparse/non-relevant features are feature agglomerated\n",
    "    else:        \n",
    "        \n",
    "        np.save('sample_feature_array',feature_array_s)\n",
    "        print(feature_array_s.shape)\n",
    "        \n",
    "        print()\n",
    "        # fit data\n",
    "        cluster.fit(feature_array_s)                \n",
    "        X_reduced = cluster.transform(feature_array_s)\n",
    "\n",
    "        # examine labels\n",
    "        labels = cluster.labels_\n",
    "        argsort_labels = np.argsort(labels)\n",
    "        \n",
    "        # plot feature clusters\n",
    "        line_plot(labels[argsort_labels], s_names[argsort_labels], [\"b\",\"g\",\"r\",\"c\",\"m\"])\n",
    "        \n",
    "        # assign agglomerated features to feature array\n",
    "        feature_array_s = X_reduced\n",
    "    \n",
    "    print('Clustered features: {}'.format(X_reduced.shape[1]))\n",
    "\n",
    "    #### define scaler\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "    # scale features again\n",
    "    feature_array_s = scaler.fit_transform(feature_array_s)\n",
    "    feature_array_n = scaler.fit_transform(feature_array_n)\n",
    "\n",
    "    # both rel/non-sparse or non-rel/parse are used, merge the two feature sets\n",
    "    if not only_agglo_f and not only_non_agglo_f:\n",
    "        \n",
    "        # merge sparse features and non-sparse features\n",
    "        feature_array_final = np.concatenate((feature_array_s,feature_array_n), axis=1)\n",
    "        f_names = np.concatenate((s_names, n_names))\n",
    "    \n",
    "    # if only sparse/non-rel features are feature agglomerated\n",
    "    if only_agglo_f:\n",
    "        \n",
    "        # put together final feature array\n",
    "        f_names = s_names\n",
    "        feature_array_final = feature_array_s\n",
    "               \n",
    "    # if only relevant/non-sparse features are feature agglomerated\n",
    "    if only_non_agglo_f:\n",
    "       \n",
    "        # put together final feature array\n",
    "        f_names = n_names\n",
    "        feature_array_final = feature_array_n\n",
    "\n",
    "        \n",
    "    print('Final number of features: {}'.format(feature_array_final.shape[1]))\n",
    "          \n",
    "    return feature_array_final, f_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(feature_array_final, solution_array, linearSVM=True):\n",
    "    \n",
    "    \"\"\"Train classifier when giving feature array and solution array;\n",
    "    linear or C-SVC; works for both with and without feature agglomeration)\n",
    "    \n",
    "    Args:       feature_array_final (np.array)\n",
    "                solution_array (np.array)\n",
    "                linearSVM (Boolean)\n",
    "                 \n",
    "    Returns:    classifier (sklearn.svm.classes.LinearSVC/SVC)\n",
    "                accuracies (np.array)\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    # split data\n",
    "    f_train, f_test, s_train, s_test = sklearn.model_selection.train_test_split(feature_array_final, \n",
    "                                                   solution_array,                                                                             \n",
    "                                                   train_size=TRAIN_SIZE,\n",
    "                                                   stratify=solution_array,\n",
    "                                                   random_state=RANDOM_STATE)\n",
    "    \n",
    "    \n",
    "    print('Training samples:', f_train.shape[0])\n",
    "    print('Test samples:', f_test.shape[0])\n",
    "    # get labels    \n",
    "    labels = np.unique(s_train,return_counts=True)[0]\n",
    "    \n",
    "    ## train classifier\n",
    "    classifier = svm.LinearSVC(random_state=RANDOM_STATE, tol=0.001, C=1.7)\n",
    "    if linearSVM==False:\n",
    "        classifier = svm.SVC(random_state=RANDOM_STATE) # non-linear\n",
    "        \n",
    "    # fit classifier\n",
    "    classifier.fit(f_train, s_train)\n",
    "    \n",
    "    # cross validation classifier\n",
    "    cv_classifier = svm.LinearSVC(random_state=RANDOM_STATE,tol=1.0, C=2.2)\n",
    "    \n",
    "    if linearSVM == False:    \n",
    "        # non-linear\n",
    "        cv_classifier = svm.SVC(random_state=RANDOM_STATE) \n",
    "\n",
    "    # cross validation\n",
    "    cv_ = sklearn.model_selection.ShuffleSplit(n_splits = 50, train_size=TRAIN_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    # get scores\n",
    "    accuracies = sklearn.model_selection.cross_val_score(cv_classifier, \n",
    "                                                feature_array_final, solution_array, \n",
    "                                                cv=cv_, scoring='accuracy')   \n",
    "    \n",
    "    precision_m = sklearn.model_selection.cross_val_score(cv_classifier, feature_array_final, \n",
    "                                                        solution_array, cv=cv_, \n",
    "                                                        scoring='precision_weighted')\n",
    "\n",
    "    recall_m = sklearn.model_selection.cross_val_score(cv_classifier, feature_array_final, \n",
    "                                                        solution_array, cv=cv_, \n",
    "                                                        scoring='recall_weighted')\n",
    "\n",
    "    f1_m = sklearn.model_selection.cross_val_score(cv_classifier, feature_array_final, \n",
    "                                                        solution_array, cv=cv_, \n",
    "                                                        scoring='f1_weighted')\n",
    "\n",
    "\n",
    "    # printing\n",
    "\n",
    "    print('\\n~~~~~ ~~~~~ ~~~~~ ~~~~~ ~~~~~ ')\n",
    "    print('\\nAccuracies:')\n",
    "    print('Mean:',np.mean(accuracies))\n",
    "    print('STD: ',np.std(accuracies))\n",
    "\n",
    "    print('\\nWeighted precision')\n",
    "    print('Mean:',np.mean(precision_m))\n",
    "    print('STD: ',np.std(precision_m))\n",
    "\n",
    "    print('\\nWeighted recall:')\n",
    "    print('Mean:',np.mean(recall_m))\n",
    "    print('STD: ',np.std(recall_m))\n",
    "\n",
    "    print('\\nWeighted F1:')\n",
    "    print('Mean:',np.mean(f1_m))\n",
    "    print('STD: ',np.std(f1_m))\n",
    "    \n",
    "    return classifier, np.mean(accuracies), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_processing(feature_array_s, feature_array_n, s_names, n_names, \n",
    "                          only_agglo_f=False, only_non_agglo_f=False):\n",
    "    \n",
    "    \"\"\"Normal processing for version without feature agglomeration;\n",
    "    combines feature array and scales featues\n",
    "    \n",
    "    Args:       feature_array_s (np.array)\n",
    "                feature_array_n (np.array)\n",
    "                s_names (np.array)\n",
    "                n_names (np.array)\n",
    "                only_agglo_f (Boolean)\n",
    "                only_non_agglo_f (Boolean)\n",
    "                 \n",
    "    Returns:    classifier (sklearn.classifier)\n",
    "                f_names (np.array)\n",
    "    \"\"\"\n",
    "    \n",
    "    # put together final feature array\n",
    "    \n",
    "    if not only_agglo_f and not only_non_agglo_f:\n",
    "        # merge sparse features and non-sparse features\n",
    "        feature_array_final = np.concatenate((feature_array_s,feature_array_n), axis=1)\n",
    "        f_names = np.concatenate((s_names, n_names))\n",
    "        \n",
    "    if only_agglo_f:        \n",
    "        f_names = s_names\n",
    "        feature_array_final = feature_array_s\n",
    "               \n",
    "    if only_non_agglo_f:        \n",
    "        f_names = n_names\n",
    "        feature_array_final = feature_array_n\n",
    "                \n",
    "    #### Scale features\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    feature_array_final = scaler.fit_transform(feature_array_final)\n",
    "    \n",
    "    print('Final number of features: {}'.format(feature_array_final.shape[1]))\n",
    "\n",
    "    return feature_array_final, f_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand selected sparse features (not used here!)\n",
    "selected_sparse_features_names = np.array(['pers pron count','refl pron count','modal count','split verbs count',\n",
    "                        'gen mod count','prep with gen count', 'dat obj count','all gens count',\n",
    "                        'gen+prep obj count','pqp count','past count', 'perfect count',\n",
    "                        'all perfect counts','praet aux count','futur 1 count','futur 2 count','all futurs count',\n",
    "                        'konj 1 count','konj 2 count','konj aux count','ind speech count','irrealis count',\n",
    "                        'all konjs count','imperative count','passive count','passive w agens','comp count',\n",
    "                        'sup count','comp+sup count','rel count','part pres count','part praet count',\n",
    "                        'all part count',\n",
    "                        'es count','brauchen count','lassen count','brauchen/lassen count','adversative','local',\n",
    "                        \"question words count\",'question marks count',\"prep obj count\",\n",
    "                        \"subjunctions\"])       \n",
    "\n",
    "print('Hand selected sparse features: {}\\n'.format(selected_sparse_features_names.shape[0]))\n",
    "selected_sparse_features_indices = np.in1d(feature_names,selected_sparse_features_names)\n",
    "selected_sparse_features_names = feature_names[selected_sparse_features_indices]\n",
    "selected_sparse_features = feature_array[:, selected_sparse_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features with zeros\n",
    "f_zeros = (feature_array == 0).sum(0)\n",
    "\n",
    "# get sparse features\n",
    "sparse_features = (f_zeros >= feature_array.shape[0]*0.33)\n",
    "\n",
    "# indices of sparse and non-sparse features\n",
    "sparse_index = np.where(sparse_features)[0]\n",
    "non_sparse_index = np.where(np.invert(sparse_features))[0]\n",
    "\n",
    "# print number of sparse and non-sparse features; print sum\n",
    "print('Sparse features\\n------------------')\n",
    "print('All features: {}'.format(len(feature_names[sparse_index])+len(feature_names[non_sparse_index])))\n",
    "print('Sparse features: {}'.format(len(feature_names[sparse_index])))\n",
    "print('Non-sparse features: {}'.format(len(feature_names[non_sparse_index])))\n",
    "\n",
    "#print('\\nNon-sparse features:\\n {}'.format(feature_names[non_sparse_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sorted indices for relevant features\n",
    "\n",
    "if literature_version:\n",
    "    sort_idx = np.load('./outputs_lit/sorted-idx.npy')\n",
    "else:\n",
    "    sort_idx = np.load('./outputs/sorted-idx.npy')\n",
    "    \n",
    "    \n",
    "#sparsity_selected = False  # not used\n",
    "#sparsity = False\n",
    "#rel_features = True\n",
    "\n",
    "#feat_agglo = True\n",
    "#n_clusters = 5\n",
    "\n",
    "#only_non_agglo_f = False\n",
    "#only_agglo_f = False\n",
    "\n",
    "print('Original shape: {}'.format(feature_array.shape))\n",
    "\n",
    "# if features are selected by sparsity\n",
    "if sparsity and not rel_features:\n",
    "    \n",
    "    # sparse features and names\n",
    "    feature_array_s = feature_array[:,sparse_index]\n",
    "    s_names = feature_names[sparse_features]\n",
    "    \n",
    "    # non-sparse features and names\n",
    "    feature_array_n = feature_array[:,non_sparse_index]    \n",
    "    n_names = feature_names[non_sparse_index]\n",
    "    \n",
    "    print('Sparse features: {}'.format(feature_array_s.shape[1]))\n",
    "    print('Non-sparse features: {}'.format(feature_array_n.shape[1]))\n",
    "        \n",
    "# if hand selected sparse feature list is used\n",
    "if sparsity_selected:\n",
    " \n",
    "    feature_array_s = feature_array[:,real_sparse_features_indices]\n",
    "    s_names = feature_names[real_sparse_features_indices]\n",
    "    \n",
    "    feature_array_n = feature_array[:,np.invert(real_sparse_features_indices)]\n",
    "    n_names = feature_names[np.invert(real_sparse_features_indices)]\n",
    "    \n",
    "    print('Sparse features: {}'.format(feature_array_s.shape[1]))\n",
    "    print('Non-sparse features: {}'.format(feature_array_n.shape[1]))\n",
    "    \n",
    "# if relevant feature sortidx is used\n",
    "if rel_features:\n",
    "    \n",
    "    # less-rel features and names\n",
    "    feature_array_s = feature_array[:,sort_idx[N_REL_FEATURES:]]\n",
    "    s_names = feature_names[sort_idx[N_REL_FEATURES:]]    \n",
    "        \n",
    "    # relevant features and names\n",
    "    feature_array_n = feature_array[:,sort_idx[:N_REL_FEATURES]]   \n",
    "    n_names = feature_names[sort_idx[:N_REL_FEATURES]]\n",
    "    \n",
    "    print('Less relevant features: {}'.format(feature_array_s.shape[1]))\n",
    "    print('Relevant features: {}\\n'.format(feature_array_n.shape[1]))\n",
    "       \n",
    "\n",
    "# if feature agglomeration is used\n",
    "if feat_agglo:\n",
    "    feature_array_final, f_names = feature_agglomeration(n_clusters, \n",
    "                                                         feature_array_s, \n",
    "                                                         feature_array_n, \n",
    "                                                         s_names, n_names, \n",
    "                                                         only_agglo_f=only_agglo_f,\n",
    "                                                         only_non_agglo_f=only_non_agglo_f)\n",
    "# without feature agglomeration\n",
    "else:\n",
    "    feature_array_final, f_names = normal_processing(feature_array_s, \n",
    "                                                     feature_array_n, \n",
    "                                                     s_names, n_names, \n",
    "                                                     only_agglo_f=only_agglo_f, \n",
    "                                                     only_non_agglo_f=only_non_agglo_f)\n",
    "\n",
    "# train classifier\n",
    "classifier,_, labels = train_classifier(feature_array_final, solution_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_by_relevance(classifier, feature_names):\n",
    "    \"\"\"Plot the features sorted by relevance\n",
    "    \n",
    "    Args:     classifier (sklearn.svm.classes.LinearSVC)\n",
    "              feature_names (np.array) \n",
    "    \"\"\"\n",
    "    \n",
    "    # sorted indices (feature with highest coefficients first)\n",
    "    sort_idx = np.argsort(-abs(classifier.coef_).max(axis=0))\n",
    "    \n",
    "    assert (len(feature_names)) == len(sort_idx) \n",
    "    \n",
    "    # get sorted coefficients and feature names\n",
    "    sorted_coef = classifier.coef_[:,sort_idx]\n",
    "    sorted_fnames = feature_names[sort_idx]\n",
    "\n",
    "    # plot feature coefficients\n",
    "    x_fig = plt.figure(figsize=(18,5))\n",
    "    plt.imshow(sorted_coef, interpolation='none', cmap='seismic',vmin=-2.5, vmax=2.5)\n",
    "    plt.colorbar()\n",
    "    plt.gca().set_yticks(range(len(labels)))\n",
    "    plt.gca().set_yticklabels(labels)\n",
    "\n",
    "    plt.gca().set_xticks(range(len(feature_names)))\n",
    "    plt.gca().set_xticklabels(sorted_fnames)\n",
    "\n",
    "    x_fig.autofmt_xdate()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# get feature names for feat agglo (sparse or less rel f)\n",
    "all_f_names = [np.array(['blue','green','red','cyan','magenta']),n_names] \n",
    "feature_names = np.concatenate(all_f_names)\n",
    "\n",
    "# if only less relevant features are used\n",
    "if only_agglo_f:\n",
    "    \n",
    "    if feat_agglo:        \n",
    "        feature_names = np.array(['blue','green','red','cyan','magenta'])\n",
    "    else:\n",
    "        feature_names = s_names\n",
    "        \n",
    "# if only relevant features\n",
    "if only_non_agglo_f:\n",
    "        feature_names = f_names\n",
    "        \n",
    "# plot\n",
    "plot_features_by_relevance(classifier, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cluster_n():\n",
    "    \"\"\"\n",
    "    Plots the accuracy for different cluster sizes\n",
    "    \"\"\"\n",
    "    all_accuracies = []\n",
    "    for i in range(1,20):\n",
    "        feature_array_final, f_names = feature_agglomeration(i,\n",
    "                                                         feature_array_s, \n",
    "                                                         feature_array_n, \n",
    "                                                         s_names, n_names, \n",
    "                                                         only_agglo_f=only_agglo_f,\n",
    "                                                         only_non_agglo_f=only_non_agglo_f)\n",
    "        \n",
    "        \n",
    "        _, acc = train_classifier(feature_array_final, solution_array)\n",
    "        all_accuracies.append(acc)\n",
    "\n",
    "    plt.plot(range(1,20), all_accuracies)\n",
    "    plt.show()\n",
    "    \n",
    "if find_cluster_n:\n",
    "    find_best_cluster_n()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_para_screening():\n",
    "    \"\"\"\n",
    "    Parameter screening for linear support vector classification\n",
    "    \"\"\"\n",
    "\n",
    "    linear_para_dict = {'C':[0.1,0.5,1.0,1.5,2.0,2.5],\n",
    "                    'tol':[0.0001,0.001,0.01,0.1,1,0.00001]}\n",
    "    \n",
    "    cv_ = sklearn.model_selection.ShuffleSplit(n_splits = 50, train_size=TRAIN_SIZE, random_state=RANDOM_STATE)\n",
    "    cv_classifier = svm.LinearSVC()\n",
    "    para_search  = sklearn.model_selection.GridSearchCV(cv_classifier, linear_para_dict, cv=cv_)\n",
    "    para_search.fit(feature_array_final, solution_array)\n",
    "    print(para_search.best_score_)\n",
    "    print(para_search.best_params_)\n",
    "    \n",
    "if para_screening:\n",
    "    linear_para_screening()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
